{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c4e758",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "### JP Maestas\n",
    "### OCT 26, 2025\n",
    "\n",
    "The main purpose of this notebook is to gather and collect datasets for our analysis. I will mainly be using the [Pandas](https://pandas.pydata.org/docs/user_guide/index.html) package in Python, as well as some other libraries to conduct statistical testing. This file may require you to download some software in order to run the code, but IPYNB files are nice in that it will let you see the output without it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # web scraping package (useful for downloading)\n",
    "import requests            # html query package (useful for interacting with html source code)\n",
    "import time \n",
    "import geopandas as gpd\n",
    "from shapely import geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693d7d9",
   "metadata": {},
   "source": [
    "# UNHCR\n",
    "[Refugee Statistics](https://www.unhcr.org/refugee-statistics)\n",
    "# Global Human Settlement \n",
    "[1975 - 2030](https://human-settlement.emergency.copernicus.eu/ghs_pop2023.php)\n",
    "# Global History Climatology Network daily\n",
    "[Daily Climate Surveys](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily)\n",
    "# 2024 Energy Usage Report\n",
    "Study on 2024 Data Center usage from [Berkley Lab](https://eta.lbl.gov/publications/2024-lbnl-data-center-energy-usage-report)\n",
    "# US Data Center Locator \n",
    "[Geolocation of Data Centers](https://map.datacente.rs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026fd7b",
   "metadata": {},
   "source": [
    "For the first part of the projec, I am going to scrape the geolocation of the data centers (nationally), and create a plot with geopandas package. Further, I am going to query the list of ZIP codes so we can perform further analysis on the impact in these regions\n",
    "\n",
    "\n",
    "NOTE! it's always important to read the [robots.txt file](https://www.datacentermap.com/robots.txt) before scraping so you don't violate their terms of service! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed96d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "base_url = \"https://www.datacentermap.com\"\n",
    "usa_url = f\"{base_url}/usa/\"\n",
    "headers = {\"User-Agent\": \"JPMM Research_Scraper\"}\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in background\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Load page\n",
    "driver.get(\"https://www.datacentermap.com/usa/\")\n",
    "time.sleep(3)  # Wait for JS to load\n",
    "\n",
    "# Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Extract state links\n",
    "state_links = []\n",
    "for a in soup.select(\"div#content a\"):\n",
    "    href = a.get(\"href\")\n",
    "    if href and \"/usa/\" in href and href.count(\"/\") == 3:\n",
    "        state_links.append(\"https://www.datacentermap.com\" + href)\n",
    "\n",
    "print(state_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbcfeb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                     0\n",
      "datacenter_name                                         Kanobe Bothell\n",
      "datacenter_city                                                Bothell\n",
      "datacenter_country                                       United States\n",
      "datacenter_address                  3301 Monte Villa Parkway Suite 125\n",
      "datacenter_website               http://www.kanobe.com/colocation.html\n",
      "datacenter_email                                      sales@kanobe.com\n",
      "datacenter_tel                                            425.686.7700\n",
      "company_name                                                    Kanobe\n",
      "company_description  Since 2009, KANOBE has been providing Business...\n",
      "company_website                                  http://www.kanobe.com\n",
      "company_email                                         sales@kanobe.com\n",
      "uuid                              0744b368-b62b-11e5-ad0b-02b4d6763261\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "url = \"https://map.datacente.rs/api/dc-summary/0744b368-b62b-11e5-ad0b-02b4d6763261\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 ( Intel Mac OS X 10_15_7)\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "data = r.json()\n",
    "\n",
    "company = data.get(\"company\", {})\n",
    "dc = data.get(\"dc\", {})\n",
    "\n",
    "record = {\n",
    "    \"datacenter_name\": dc.get(\"Name\"),\n",
    "    \"datacenter_city\": dc.get(\"City\") or company.get(\"city\"),\n",
    "    \"datacenter_country\": dc.get(\"Country\") or company.get(\"country\"),\n",
    "    \"datacenter_address\": dc.get(\"Address\") or company.get(\"address\"),\n",
    "    \"datacenter_website\": dc.get(\"Website\") or company.get(\"website\"),\n",
    "    \"datacenter_email\": dc.get(\"Email\") or company.get(\"email\"),\n",
    "    \"datacenter_tel\": dc.get(\"Tel\") or company.get(\"tel\"),\n",
    "    \"company_name\": company.get(\"name\"),\n",
    "    \"company_description\": company.get(\"short_description\"),\n",
    "    \"company_website\": company.get(\"website\"),\n",
    "    \"company_email\": company.get(\"email\"),\n",
    "    \"uuid\": dc.get(\"uuid\"),\n",
    "}\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = pd.DataFrame([record])\n",
    "print(df.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f0612",
   "metadata": {},
   "source": [
    "This scraper finds all the url requests and collects metadata about "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D3C",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
